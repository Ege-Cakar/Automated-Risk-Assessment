from typing import List, Optional, Dict, Any, Union, Sequence
from autogen_agentchat.agents import BaseChatAgent
from autogen_agentchat.base import Response, TaskResult
from autogen_agentchat.messages import TextMessage, ChatMessage, StopMessage, HandoffMessage
from autogen_agentchat.teams import RoundRobinGroupChat
from autogen_agentchat.conditions import TextMentionTermination, MaxMessageTermination
from autogen_core import CancellationToken
from autogen_core.models import ChatCompletionClient
import asyncio
import logging
from src.custom_autogen_code.lobe import Lobe
from src.utils.db_loader import LobeVectorMemory, LobeVectorMemoryConfig
from src.utils.streaming_utils import AgentStreamer, print_agent_header

logger = logging.getLogger(__name__)

class Expert(BaseChatAgent):
    """
    Expert agent that internally manages a team of two Lobe agents.
    Appears as a single agent externally but runs an internal deliberation process.
    """
    
    def __init__(
        self,
        name: str,
        model_client: ChatCompletionClient,
        vector_memory: 'LobeVectorMemory',
        system_message: str = None,
        lobe1_config: Dict[str, Any] = None,
        lobe2_config: Dict[str, Any] = None,
        max_rounds: int = 100,
        description: str = "An expert agent that internally deliberates using two specialized lobes, one for creativity and one for reasoning.",
        **kwargs
    ):
        """
        Initialize Expert agent with two internal Lobe agents.
        
        Args:
            name: Expert agent name
            model_client: The model client for both lobes
            vector_memory: Shared vector memory for both lobes
            lobe1_config: Configuration for Lobe 1 (keywords, temperature, system_message, tools)
            lobe2_config: Configuration for Lobe 2 (keywords, temperature, system_message, tools)
            max_rounds: Maximum rounds of internal discussion
            description: Agent description
            **kwargs: Additional arguments
        """
        super().__init__(name=name, description=description)
        
        self._model_client = model_client
        self._vector_memory = vector_memory
        self._max_rounds = max_rounds
        self._base_system_message = system_message if system_message else (
            "You are an expert assistant with deep knowledge in your domain. "
            "You think carefully and provide well-reasoned responses."
        )
        
        # Default configurations
        lobe1_config = lobe1_config or {}
        lobe2_config = lobe2_config or {}

        lobe1_specific = lobe1_config.get('system_message', 
            "You are the creative counterpart, the left lobe. Brainstorm and generate innovative ideas, make them not too outlandish, but your main goal is to generate as many creative ideas as possible. A more grounded agent, your reason counterpart, will be sifting through them. However, ALWAYS STAY ON TOPIC AND DO NOT STRAY FROM YOUR TASK.")
        
        lobe2_specific = lobe2_config.get('system_message',
            "You are the reason counterpart, the right lobe. Challenge assumptions, identify gaps, "
            "and synthesize conclusions from the discussion. Your main task is to look at the ideas generated by the creative counterpart, the left lobe, and filter outlandish ones while letting good ideas flow."
            "When you believe a satisfactory "
            "conclusion has been reached, start your message with 'CONCLUDE:' followed by a "
            "comprehensive summary of the key insights and final answer.")
        

        lobe1_full_message = f"{self._base_system_message}\n\n{lobe1_specific}"
        lobe2_full_message = f"{self._base_system_message}\n\n{lobe2_specific}"

        # Create Lobe 1 - Analytical specialist
        self._lobe1 = Lobe(
            name=f"{name}_creative",
            model_client=model_client,
            vector_memory=vector_memory,
            keywords=lobe1_config.get('keywords', []),
            temperature=lobe1_config.get('temperature', 0.8),
            system_message=lobe1_full_message,
            tools=lobe1_config.get('tools', None)
        )
        
        self._lobe2 = Lobe(
            name=f"{name}_reason",
            model_client=model_client,
            vector_memory=vector_memory,
            keywords=lobe2_config.get('keywords', []),
            temperature=lobe2_config.get('temperature', 0.4),  # Lower temperature for checking
            system_message=lobe2_full_message,
            tools=lobe2_config.get('tools', None)
        )
        
        # Initialize contexts for both lobes
        self._initialized = False
        
        # Setup internal team
        self._setup_internal_team()
    
    async def _initialize_lobes(self):
        """Initialize context for both lobes."""
        if not self._initialized:
            await self._lobe1.initialize_context()
            await self._lobe2.initialize_context()
            self._initialized = True
            logger.info(f"Initialized both lobes for Expert {self.name}")
    
    def _setup_internal_team(self):
        """Set up the internal round-robin team for the two lobes."""
        # Use TextMentionTermination to stop when Lobe2 says "CONCLUDE:"
        conclude_condition = TextMentionTermination("CONCLUDE:")
        
        # Fallback: stop after max rounds
        max_messages_condition = MaxMessageTermination(max_messages=self._max_rounds * 2)
        
        # Create internal team
        self._internal_team = RoundRobinGroupChat(
            participants=[self._lobe1, self._lobe2],
            termination_condition=conclude_condition | max_messages_condition
        )
        
        logger.info(f"Setup internal team for Expert {self.name}")
    
    async def on_messages(
        self, 
        messages: List[ChatMessage], 
        cancellation_token: CancellationToken
    ) -> Response:
        """
        Process incoming messages by running internal team discussion with streaming.
        
        Args:
            messages: List of chat messages
            cancellation_token: Cancellation token
            
        Returns:
            Response containing the expert's synthesized conclusion
        """
        # Print agent header
        print_agent_header(f"Expert {self.name}")
        
        # Start streaming with thinking indicator
        async with AgentStreamer(f"Expert {self.name}") as streamer:
            # Ensure lobes are initialized
            await self._initialize_lobes()
            
            # Extract the query from the last message
            last_message = messages[-1]
            if isinstance(last_message, TextMessage):
                query = last_message.content
            else:
                query = str(last_message)
            
            # Log the incoming query
            logger.info(f"Expert {self.name} received query: {query}")
            
            # Prepare initial task for internal team
            internal_task = f"Please analyze and respond to this query: {query}"
            
            try:
                # Run internal team discussion
                logger.info(f"Starting internal deliberation for Expert {self.name}")
                
                # Use the internal team's run_stream method if available for real streaming
                if hasattr(self._internal_team, 'run_stream'):
                    # Real streaming from internal team
                    stream = self._internal_team.run_stream(
                        task=internal_task,
                        cancellation_token=cancellation_token
                    )
                    
                    # Collect the streamed response
                    final_response = ""
                    async for message in stream:
                        if hasattr(message, 'content'):
                            content = message.content
                            final_response += content
                            # Stream each chunk as it comes
                            print(content, end="", flush=True)
                    
                    print()  # New line after streaming
                else:
                    # Fallback to regular run method
                    result = await self._internal_team.run(
                        task=internal_task,
                        cancellation_token=cancellation_token
                    )
                    
                    # Extract and format the conclusion
                    final_response = self._extract_conclusion(result)
                    
                    # Display the response (non-streaming fallback)
                    await streamer.stream_response(final_response)
                
                logger.info(f"Expert {self.name} completed deliberation")
                
                # Return response as coming from the Expert
                return Response(
                    chat_message=TextMessage(
                        content=final_response,
                        source=self.name
                    )
                )
                
            except asyncio.CancelledError:
                logger.warning(f"Expert {self.name} deliberation was cancelled")
                await streamer.stream_response("[Deliberation cancelled]")
                raise
                
            except Exception as e:
                logger.error(f"Error in Expert {self.name} deliberation: {str(e)}", exc_info=True)
                error_message = (
                    f"I encountered an error during internal deliberation. "
                    f"Let me provide a direct response instead: Based on the query about "
                    f"'{query}', I need more specific information to provide a detailed answer."
                )
                await streamer.stream_response(error_message)
                return Response(
                    chat_message=TextMessage(
                        content=error_message,
                        source=self.name
                    )
                )
    
    def _extract_conclusion(self, task_result: TaskResult) -> str:
        """
        Extract the conclusion from the internal team's discussion.
        
        Args:
            task_result: Result from the internal team discussion
            
        Returns:
            The synthesized conclusion string
        """
        # Look for explicit conclusion from Lobe 2
        for message in reversed(task_result.messages):
            if isinstance(message, TextMessage) and message.source == self._lobe2.name:
                content = message.content
                if "CONCLUDE:" in content:
                    # Extract everything after "CONCLUDE:"
                    conclusion_start = content.find("CONCLUDE:") + len("CONCLUDE:")
                    conclusion = content[conclusion_start:].strip()
                    return conclusion
        
        # Fallback: If no explicit conclusion, synthesize from last few messages
        logger.warning(f"No explicit conclusion found for Expert {self.name}, synthesizing...")
        
        synthesis_parts = ["Based on internal analysis:"]
        
        # Get last 3 messages for synthesis
        recent_messages = task_result.messages[-3:] if len(task_result.messages) >= 3 else task_result.messages
        
        for msg in recent_messages:
            if isinstance(msg, TextMessage):
                content = msg.content
                synthesis_parts.append(f"- {content}")
        
        return "\n".join(synthesis_parts)
    
    async def update_keywords(self, lobe1_keywords: List[str] = None, lobe2_keywords: List[str] = None):
        """
        Update keywords for one or both lobes.
        
        Args:
            lobe1_keywords: New keywords for Lobe 1
            lobe2_keywords: New keywords for Lobe 2
        """
        if lobe1_keywords is not None:
            await self._lobe1.update_keywords(lobe1_keywords)
            logger.info(f"Updated Lobe 1 keywords for Expert {self.name}")
            
        if lobe2_keywords is not None:
            await self._lobe2.update_keywords(lobe2_keywords)
            logger.info(f"Updated Lobe 2 keywords for Expert {self.name}")
    
    async def add_knowledge(self, content: str, metadata: Dict[str, Any] = None):
        """
        Add new knowledge to the shared vector database.
        
        Args:
            content: Knowledge content to add
            metadata: Optional metadata
        """
        # Add through one of the lobes (they share the same vector memory)
        await self._lobe1.add_knowledge(content, metadata)
        logger.info(f"Added knowledge to Expert {self.name}'s shared database")
    
    @property
    def lobe1(self) -> Lobe:
        """Access to Lobe 1 for direct configuration if needed."""
        return self._lobe1
    
    @property
    def lobe2(self) -> Lobe:
        """Access to Lobe 2 for direct configuration if needed."""
        return self._lobe2
        
    async def on_reset(self, cancellation_token: CancellationToken) -> None:
        """Reset the agent's state."""
        # Reset both internal lobes
        if hasattr(self, '_lobe1') and self._lobe1:
            await self._lobe1.on_reset(cancellation_token)
        if hasattr(self, '_lobe2') and self._lobe2:
            await self._lobe2.on_reset(cancellation_token)
        
        # Reset initialization flag
        self._initialized = False
        
        # Re-setup the internal team to ensure clean state
        if hasattr(self, '_internal_team'):
            self._setup_internal_team()
        
        logger.info(f"Reset Expert {self.name}")
    
    @property
    def produced_message_types(self) -> List[type[ChatMessage]]:
        """Message types that this agent can produce."""
        # Expert can produce text messages and potentially stop messages
        return [TextMessage, StopMessage]